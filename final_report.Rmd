---
title: "Midterm Project"
author: "Linh Tran"
date: "3/29/2021"
output: html_document
---

```{r setup, include=FALSE}
set.seed(2021)

library(tidyverse)
library(caret)
library(glmnet)
library(mlbench)  
library(pROC)   
library(pdp) 
library(vip) 
library(AppliedPredictiveModeling)
library(e1071)
library(MASS) 
library(klaR)
library(lares)
library(naniar) #handling missing data
library(imbalance) #dealing with imbalanced datasets
library(gridExtra) #display plots in grids
library(patchwork)
library(randomForest)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Introduction

Describe your data set. Provide proper motivation for your work.

What questions are you trying to answer?
How did you prepare and clean the data?


## Significance and Background

According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.

Data Source: https://www.kaggle.com/fedesoriano/stroke-prediction-dataset.\

Variables included in the original data set:

- id: unique identifier
- gender: "Male", "Female" or "Other"
- age: age of the patient
- hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension
- heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease
- ever_married: "No" or "Yes"
- work_type: "children", "Govt_jov", "Never_worked", "Private" or "Self-employed"
- Residence_type: "Rural" or "Urban"
- avg_glucose_level: average glucose level in blood
- bmi: body mass index
- smoking_status: "formerly smoked", "never smoked", "smokes" or "Unknown"*
- stroke: 1 if the patient had a stroke or 0 if not
*Note: "Unknown" in smoking_status means that the information is unavailable for this patient

 
## Questions of Interest
* Comparing several models' performance. What model has the best prediction accuracy?
* What variables are significant and included in the model?

## Data Cleaning

The dataset is essentially clean and contains 12 variables (including ID) and 5110 observations. Excluding the id, we have ten predictor variables and one binary outcome variable `stroke` (0:no stroke, 1:stroke). The data set is converted to data frame for modeling purposes. Categorical variables are converted from character to factor class in order for them to be included in the model and for the purpose of analysis. I also omit the "Other" category which only includes one observation from the `gender` variable, leaving "male" and "female" as the two categories. Binary predictor variables such as `hypertension`, `heart_disease`, and `ever_married` are also recoded so that `0` means no and `1` means yes. The `work_type` variable which has 5 levels (children, govt_job, never_worked, private, and self_employed) are recoded to lower snake case. My main aim is to find out the appropriate models that have a better performance on prediction by comparing several models' performance. 


```{r}
stroke_df <- read_csv("data/healthcare-dataset-stroke-data.csv") %>% 
  janitor::clean_names()

miss_scan_count(data = stroke_df, search = list("N/A", "Unknown"))

summary(stroke_df)

```


```{r}
stroke_df$stroke = as.factor(stroke_df$stroke)

stroke_df$ever_married = as.factor(stroke_df$ever_married)
stroke_df$work_type = as.factor(stroke_df$work_type)
stroke_df$residence_type = as.factor(stroke_df$residence_type)
stroke_df$smoking_status = as.factor(stroke_df$smoking_status)
stroke_df$heart_disease = as.factor(stroke_df$heart_disease)
stroke_df$hypertension = as.factor(stroke_df$hypertension)
stroke_df$work_type = as.factor(stroke_df$work_type)
stroke_df$bmi = as.numeric(stroke_df$bmi)

stroke_df = stroke_df %>% 
    filter(gender != "Other") %>%
    mutate(gender = as.factor(gender)) %>% 
    mutate(ever_married = recode(ever_married, No = "0", Yes = "1")) %>% 
    mutate(work_type = recode(work_type, 
                            `children` = "children", 
                            `Govt_job` = "govt_job",
                            `Never_worked` = "never_worked",
                            `Private` = "private",
                            `Self-employed` = "self_employed"))  %>% 
  dplyr::select(-id) 


stroke_df_clean <- replace_with_na(data = stroke_df, replace = list(bmi = c("N/A"), smoking_status = c("Unknown"))) 

x <- model.matrix(stroke ~ ., data = stroke_df)[,-11]
y <- stroke_df$stroke
```


# Exploratory analysis/visualization

Is there any interesting structure present in the data?
What were your findings?
Here you can use any techniques as long as they are adequately explained. If you cannot find anything interesting, then describe what you tried and show that there isnâ€™t much visible structure. Data science is NOT manipulating the data in some way until you get an answer.

THere are 1544 "Unknown" values for `smoking_status` variable and 201 missing values for `bmi` variable. First, lets replace those values with NAs. 

```{r}
stroke_df_clean = replace_with_na(data = stroke_df, 
                                  replace = list(bmi = c("N/A"), smoking_status = c("Unknown"))) %>% 
  mutate(bmi = as.numeric(bmi))
  
unique(stroke_df_clean$smoking_status) #formerly smoked, never smoked, smokes, NA
```

Next we can look at the distribution of BMI

```{r}
stroke_df_clean %>% 
  ggplot(aes(x = bmi)) + geom_histogram()
```

The distribution is right-skewed. Because this is the only variable with missing data (at least the numerical variables) we can impute the `BMI` values with the mean or median.

```{r}
stroke_df_imp =
  bind_shadow(stroke_df_clean) %>% 
  impute_median_at(.vars = c("bmi")) %>% 
  add_label_shadow()

ggplot(stroke_df_imp, aes(x = bmi_NA, y = bmi)) +
  geom_boxplot() +
  ggtitle("Comparison between no-missing and imputed values for BMI")

```

Next we can look at the proportion of people who have a stroke

```{r}
stroke_df_clean %>%
  dplyr::select(stroke) %>%
  ggplot(aes(x = stroke)) +
  geom_bar() 


# Count how many people who have a stroke
stroke_df_clean %>% 
  group_by(stroke) %>% 
  summarize (n = n()) %>% 
  mutate(prop = round(n/sum(n), 2)) %>% 
  knitr::kable()


```

We see that only 5% of all the people in the dataset had a stroke at some point. This means that our baseline dummy model has an accuracy of 95%. 

We found that the stroke outcome distribution is imbalanced with 4861 observations have no stroke while 249 observations have a stroke.

There are 201 missing values in BMI. Among these missing values, 40 observations have a stroke while 161 observations without stroke. We will then apply preprocess imputation in the caret train function to address the imputation problem.

Next, the characteristics of features will help us determine which model would have better prediction accuracy. As the outcome is binary, and the features are mixtures of continuous and categorical variables. We also have to decide how to partition the train and test data, which cross-validation method to use. Evaluation metrics should be used and set up a reasonable tuning grid corresponding to the tuning parameter.



```{r}
set.seed(2021)

trRow <- createDataPartition(y = stroke_df$stroke,
                                p = 0.75,
                                list = FALSE)

train_data= stroke_df[trRow, ]
test_data = stroke_df[-trRow, ]

vis_miss(train_data)


```

### Imputation


Partition the dataset, I will use 70% as training data and 30% as test data.
```{r}
set.seed(123)
trRow = createDataPartition(y = stroke_df$stroke, p = 0.7, list = F)
train.data = stroke_df[trRow, ]
test.data = stroke_df[-trRow, ] 
```

Try imputation with `preProcess() `
```{r}
knnImp = preProcess(train.data, method = "knnImpute", k = 3)
train.data.imp = predict(knnImp, train.data)
vis_miss(train.data)
test.data.imp = predict(knnImp,test.data)
vis_miss(test.data)
```



```{r, eval = F, echo = F}

fit.lm <- train(x = train_data[1:10],
                y = train_data$stroke,
                preProcess =c("knnImpute"),
                method = "lm",
                trControl =trainControl(method = "none",
                                        preProcOptions =list(k = 5)))


knnImp = preProcess(train_data, method = "knnImpute", k = 3)

train_knn <- predict(knnImp, train_data)

test_knn <- predict(knnImp, test_data)

head(train_knn)

```



# Models

What predictor variables did you include?
What technique did you use? What assumptions, if any, are being made by using this technique?
If there were tuning parameters, how did you pick their values?
Discuss the training/test performance if you have a test data set.
Which variables play important roles in predicting the response?
What are the limitations of the models you used (if there are any)? Are the models flexible enough to capture the underlying truth?

## Generalized Linear Model (GLM)

```{r}

contrasts(stroke_df$stroke) 

glm.fit <- glm(stroke ~ ., 
               data = stroke_df, 
               subset = trRow, 
               family = binomial(link = "logit"))
               
summary(glm.fit)
```

The smallest p-value here is associated with age. The positive coefficient suggests that the older one gets, the more likely that person will have a stroke. \
For every unit change in age, the log odds of having stroke (versus not having a stroke) increase by 0.070689.
For every unit change in average glucose level, the log odds of having stroke (versus not having a stroke) increase by 0.006253.
`

We first consider the simple classifier with a cut-off of 0.5 and evaluate its performance on the test data.

We can use `predict` function to predict the probability that a person has a stroke, given values of the predictors. 
```{r}
# Convert predicted probabilities into class labels, stroke or no stroke.

glm.probs <- predict(glm.fit, newdata = test_data,
                           type = "response")
glm.pred = rep("no stroke", length(glm.probs))
glm.pred[glm.probs>0.5] = "stroke"
table(glm.pred, test_data$stroke)

# 2x2 table
confusionMatrix(data = as.factor(glm.pred),
                reference = stroke_df$stroke[-trRow],
                positive = "stroke")
              
mean(glm.pred == test_data$stroke) # 95% of cases have been correctly predicted

```


## LDA

```{r}
ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

set.seed(1)
model.lda <- train(x = stroke_df[rowTrain,1:8],
                   y = stroke_df$stroke[rowTrain],
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)
```




## QDA


```{r}
qda.fit <- qda(stroke~., data = stroke_df,
               subset = rowTrain)
               

summary(qda.fit )


qda.pred <- predict(qda.fit, newdata = dat[-rowTrain,])
head(qda.pred$posterior)

set.seed(1)
model.qda <- train(x = stroke_df[,1:10],
                   y = stroke_df$stroke,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)

```




## KNN



# Conclusions

What were your findings? Are they what you expect? What insights into the data can you make?


```{r}
stroke_df$stroke <- ifelse(stroke_df$stroke==1,'yes','no')
stroke_df$stroke = as.factor(stroke_df$stroke)
```

Split data

```{r}
set.seed(1234)
splitIndex <- createDataPartition(y =stroke_df$stroke, p = .75, list = FALSE, times = 1)
train_df <- stroke_df[ splitIndex,]
test_df  <- stroke_df[-splitIndex,]
```


In this case, weâ€™re going to cross-validate the data 3 times, therefore training it 3 times on different portions of the data before settling on the best tuning parameters.
Because this is a classification model, weâ€™re requesting that our metrics use ROC instead of the default RMSE


```{r}
objControl <- trainControl(method='cv', number=3, returnResamp='none', summaryFunction = twoClassSummary, classProbs = TRUE)

outcomeName <- "stroke"
predictorsNames <- names(stroke_df)[names(stroke_df) != outcomeName]

objModel <- train(train_df[,predictorsNames], train_df[,outcomeName], 
                  method='gbm', 
                  trControl=objControl,  
                  metric = "ROC",
                  preProc = c("center", "scale"))
```



```{r}
set.seed(1)


stroke2 <- model.matrix(stroke ~ ., stroke_df)[ ,-1]


trainRows <- createDataPartition(y = stroke_df$stroke, p = 0.7, list = FALSE)

stroke_df_2 <- model.matrix(Salary ~ ., Hitters)[ ,-1]
# matrix of predictors (glmnet uses input matrix)
x <- Hitters2[trainRows,]
# vector of response
y <- Hitters$Salary[trainRows]

corrplot(cor(x), method = "circle", type = "full")
```


```{r}
# predict using test data
glmnet_pred <- predict(glmnet_model, newdata = mm_test) 

# plot ROC curve and report AUC
roc.glm <- roc(mm_test$stroke,glmnet_pred)
roc.glm  #AUC = 0.859
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```

```{r}
stroke_test = stroke_test %>% 
  dplyr::select(-id)
roc.rf <- roc(stroke_test$stroke[-trRow], rf_pred_original)
```

